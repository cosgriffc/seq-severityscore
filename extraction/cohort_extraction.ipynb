{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Extraction Notebook\n",
    "### Christopher V. Cosgriff, MPH\n",
    "#### NYU School of Medicine\n",
    "<hr />\n",
    "In this notebook we will extract the necessary variables for feature engineering and cohort construction to build a set of severity score models for this study.\n",
    "\n",
    "\n",
    "The initial variables we will extract are as follows:\n",
    "* Age, gender, weight, ethnicity (`patient` table)\n",
    "* Source of admission, unit type (`patient` table)\n",
    "* Laboratory data on the first day  (`labsfirstday` materialized view)\n",
    "    * Blood gases: PaO2, pH, base excess, bicarbonate,\n",
    "    * Hematology: hematocrit, hemoglobin, lymphocytes, neutrophils, platelets, white cell count\n",
    "    * Electrolytes: calcium, chloride, ionized calcium, magnesium, phosphate, sodium\n",
    "    * Biochemistry: albumin, amylase, bilirubin, blood urea nitrogen (BUN), B-natriuretic peptide, creatine phosphokinase (cpk), creatinine, lactate, lipase, troponin I/T, pH, bicarbonate, base excess, glucose\n",
    "    * Coagulation: PT/INR, fibrinogen\n",
    "* Vital signs on the first day (`vitalsfirstday` materialized view)\n",
    "    * Heart rate\n",
    "    * Blood pressure\n",
    "    * Respiratory rate\n",
    "    * SpO2\n",
    "* Treatments (`treatmentfirstday` materializd view)\n",
    "    * Antiarrhythmics\n",
    "    * Antibiotics\n",
    "    * Vasopressors\n",
    "    * Sedatives\n",
    "    * Diuretics\n",
    "    * Blood products\n",
    "* Ventilation status (`apachepredvar` table)\n",
    "* Admission Dx (`APACHE_GROUPS` materialized view)\n",
    "* APACHE IVa Features (`apachepredvar` table)\n",
    "\n",
    "The _label_ for our classifier as well as their APACHE IVa predicted mortality are located in `apachepatientresult`.\n",
    "\n",
    "## 0 - Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dbname = 'eicu'\n",
    "schema_name = 'eicu_crd'\n",
    "query_schema = 'SET search_path TO ' + schema_name + ';'\n",
    "con = psycopg2.connect(dbname=dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Materialized Views\n",
    "\n",
    "We will generate the requisite materialized views to aid in the extraction of the cohort features. We start by introducing helper functions for interacting with the eICU-CRD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_safely(sql, con):\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "    except:\n",
    "        cur.execute('rollback;')\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "    return\n",
    "\n",
    "def generate_materialized_view(query_file, con, query_schema):\n",
    "    with open(query_file) as fp:\n",
    "        query = ''.join(fp.readlines())\n",
    "    print('Generating materialized view using {} ...'.format(query_file), end=' ')\n",
    "    execute_query_safely(query_schema + query, con)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the materialized views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating materialized view using ./sql/vitalsfirstday.sql ... done.\n",
      "Generating materialized view using ./sql/labsfirstday.sql ... done.\n",
      "Generating materialized view using ./sql/treatmentfirstday.sql ... done.\n",
      "Generating materialized view using ./sql/apache-groups.sql ... done.\n"
     ]
    }
   ],
   "source": [
    "generate_materialized_view('./sql/vitalsfirstday.sql', con, query_schema)\n",
    "generate_materialized_view('./sql/labsfirstday.sql', con, query_schema)\n",
    "generate_materialized_view('./sql/treatmentfirstday.sql', con, query_schema)\n",
    "generate_materialized_view('./sql/apache-groups.sql', con, query_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading a base cohort. As our most import exclusion criteria is whether or not they had an APACHE IVa score (so we can fit models in the subpopulations), we can join the `patient` table on the `apachepredvar` and `apachepatientresult` table. There are 200,859 ICU stays, and so the number of rows returned will be the remainder after exclusion of patients for which APACHE data or an APACHE score is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136231, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cohort_query = query_schema + '''\n",
    "WITH apacheIV AS\n",
    "(\n",
    "    SELECT patientunitstayid, apachescore\n",
    "         , CAST(predictedhospitalmortality AS float) AS apache_prediction\n",
    "         , actualhospitalmortality \n",
    "    FROM apachepatientresult\n",
    "    WHERE apacheversion = 'IVa'\n",
    "    AND apachescore > 0\n",
    "    AND CAST(predictedhospitalmortality AS float) >= 0\n",
    ")\n",
    ", admit_order AS\n",
    "(\n",
    "    SELECT patientunitstayid, uniquepid\n",
    "    , ROW_NUMBER() OVER (partition BY uniquepid ORDER BY hospitaladmitoffset DESC, patientunitstayid) AS admission_num\n",
    "    FROM patient\n",
    ")\n",
    "\n",
    "SELECT p.patientunitstayid, p.age, p.gender, p.ethnicity, p.admissionheight AS height\n",
    "       , p.admissionweight AS weight , p.unittype AS unit_type, p.unitadmitsource\n",
    "       , p.unitdischargeoffset AS unit_los, p.hospitaldischargeoffset AS hospital_los\n",
    "       , a.day1meds AS gcs_meds, a.day1verbal AS gcs_verbal, a.day1motor AS gcs_motor\n",
    "       , a.day1eyes AS gcs_eyes, a.admitDiagnosis AS admit_diagnosis, ag.apachedxgroup as adx_group\n",
    "       , a.thrombolytics AS apache_thrombolytics, a.electivesurgery AS apache_elect_surg\n",
    "       , a.readmit AS apache_readmit, ao.admission_num, a.ima AS apache_ima\n",
    "       , a.midur AS apache_midur, a.oOBVentDay1 AS apache_ventday1, a.oOBIntubDay1 AS apache_intubday1\n",
    "       , a.day1fio2 AS apache_fio2, a.day1pao2 AS apache_pao2, (a.day1pao2 / a.day1fio2) AS apache_o2ratio\n",
    "       , a.ejectfx AS apache_ejectfx, a.creatinine AS apache_creatinine\n",
    "       , a.graftcount AS apache_graftcount, o.apache_prediction\n",
    "       , o.apachescore AS apache_score, o.actualhospitalmortality AS hospital_expiration\n",
    "FROM patient p\n",
    "INNER JOIN apachepredvar a\n",
    "ON p.patientunitstayid = a.patientunitstayid\n",
    "INNER JOIN apacheIV o\n",
    "ON p.patientunitstayid = o.patientunitstayid\n",
    "INNER JOIN admit_order ao\n",
    "on p.patientunitstayid = ao.patientunitstayid\n",
    "INNER JOIN APACHE_GROUPS ag\n",
    "on p.patientunitstayid = ag.patientunitstayid\n",
    "ORDER BY patientunitstayid;\n",
    "'''\n",
    "\n",
    "base_cohort = pd.read_sql_query(base_cohort_query, con)\n",
    "base_cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 200,859 patients in the database, 136,231 have APACHE IVa variables recorded as well as a hospital mortality prediction carried out. We'll then load the variables that will be used to derive the expanded feature set into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192320, 85)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_query = query_schema + '''\n",
    "SELECT v.patientunitstayid, v.HR_Mean, v.SBP_periodic_Mean, v.DBP_periodic_Mean\n",
    "    , v.MAP_periodic_Mean, v.SBP_aperiodic_Mean, v.DBP_aperiodic_Mean\n",
    "    , v.MAP_aperiodic_Mean, v.RR_Mean, v.SpO2_Mean, v.TempC_Mean\n",
    "    , ANIONGAP_min, ANIONGAP_max, ALBUMIN_min, ALBUMIN_max \n",
    "    , AMYLASE_min, AMYLASE_max, BASEEXCESS_min, BASEEXCESS_max\n",
    "    , BICARBONATE_min, BICARBONATE_max, BUN_min, BUN_max, BNP_min\n",
    "    , BNP_max, CPK_min, CPK_max, BILIRUBIN_min, BILIRUBIN_max\n",
    "    , CALCIUM_min, CALCIUM_max, IONCALCIUM_min, IONCALCIUM_max\n",
    "    , CREATININE_min, CREATININE_max, CHLORIDE_min, CHLORIDE_max\n",
    "    , GLUCOSE_min, GLUCOSE_max, HEMATOCRIT_min, HEMATOCRIT_max\n",
    "    , FIBRINOGEN_min, FIBRINOGEN_max, LIPASE_min, LIPASE_max\n",
    "    , HEMOGLOBIN_min, HEMOGLOBIN_max, LACTATE_min, LACTATE_max\n",
    "    , LYMPHS_min, LYMPHS_max, MAGNESIUM_min, MAGNESIUM_max\n",
    "    , PAO2_min, PAO2_max, PH_min, PH_max, PLATELET_min\n",
    "    , PLATELET_max, PMN_min, PMN_max, PHOSPHATE_min, PHOSPHATE_max\n",
    "    , POTASSIUM_min, POTASSIUM_max, PTT_min, PTT_max, INR_min\n",
    "    , INR_max, PT_min, PT_max, SODIUM_min, SODIUM_max\n",
    "    , TROPI_min, TROPI_max, TROPT_min, TROPT_max, WBC_min\n",
    "    , WBC_max, t.abx, t.pressor, t.antiarr, t.sedative\n",
    "    , t.diuretic, t.blood_product\n",
    "FROM vitalsfirstday v\n",
    "LEFT JOIN labsfirstday l\n",
    "ON v.patientunitstayid = l.patientunitstayid\n",
    "LEFT OUTER JOIN treatmentfirstday t\n",
    "ON v.patientunitstayid = t.patientunitstayid;\n",
    "'''\n",
    "\n",
    "feature_set = pd.read_sql_query(feature_set_query, con)\n",
    "feature_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then merge the two dataframes. Note that we are using an _inner join_ here and so if a patient did not have a vital sign recordin in `vitalsperiodic` they will not be included in the cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134946, 117)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = pd.merge(left=base_cohort, right=feature_set, how='inner', on='patientunitstayid')\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go from 136,231 to 134,946. The missing 1,285 did not have recorded vitals and their APACHE IVa score was likely derived from the `nursecharting` table which we are not utilizing in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Inclusion / Exclusion\n",
    "\n",
    "By nature of our SQL query, we have already excluded patients not eligible/capable of producing a valid score, and patients who lack all vital/lab/treatment data. We can then check the APACHE IVa criteria:\n",
    "\n",
    "1. Not readmissions\n",
    "2. Not admitted from another ICU\n",
    "3. Admitted to ICU for $\\geq4$hours\n",
    "4. Not burn patients\n",
    "5. Not transplant patients (other than hepatic renal)\n",
    "5. Age $\\geq16$\n",
    "\n",
    "__1 - Not Readmitted__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134946, 117)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.apache_readmit == 0, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Not Admitted from ICU__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134890, 117)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.unitadmitsource != 'Other ICU', :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "56 patients were admitted from another ICU, but still had scores calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - ICU LoS $\\geq$ 4h__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134890, 117)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.unit_los >= 240, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No patients had a LoS <4h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Not Burn Patients__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134890, 117)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.admit_diagnosis != 'BURN', :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no burn patients inappropriately kept in the cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Not Transplant Patients__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134890, 117)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.admit_diagnosis != 'KIDPANTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'S-KIDPTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'S-HEARTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'HEARTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'S-LUNGTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'LUNGTRAN', :]\n",
    "cohort = cohort.loc[cohort.admit_diagnosis != 'LUNGSTRAN', :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were also no transplant patients inappropriately kept in the cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Age $\\geq$ 16, and Not Missing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134890, 117)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per Rodrigo, the median age for >89 pt in eICU is 93\n",
    "cohort.loc[cohort.age == '> 89', 'age'] = 93.0\n",
    "cohort = cohort.loc[cohort.age != '', :]\n",
    "cohort.age = cohort.age.astype('float64')\n",
    "cohort = cohort.loc[cohort.age >= 16., :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No patients <16 years of age were in the dataset to be excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Cleaning, Formatting and Feature Engineering\n",
    "\n",
    "Before anything else, we can drop numerous variables from the pull as we won't need both min/max for various features (they'll often be the same value since many things are only measured once in 24 hours, and even if they're not they'll be highly correlated). Instead we'll be going by the following principle: the most _abnormal_ laboratory value in the first 24 hours of ICU stay will be included. That is we will use:\n",
    "\n",
    "* The minimum value for: bicarbonate, chloride, calcium, magnesium, base excess (including negative values), platelets, hemoglobin, phosphate, fibrinogen, pH and hematocrit\n",
    "* The maximum value for: creatinine, BUN, bilirubin, PT/INR, lactate, troponin I/T, amylase, lipase, B-natriuretic peptide and creatinine phosphokinase\n",
    "* For sodium, which aberrantly deviates bidirectionally, the most abnormal value was defined as the value with greatest deviation from the normal range boundaries.\n",
    "    * This can be applied to glucose and potassium as well.\n",
    "* For white blood cell and neutrophil counts, if any measurements were lower than the lower limit of normal, the minimum value was used; if the minimum was within normal range then the maximum was used as the most abnormal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the unidirectional abberations, just drop what isn't needed\n",
    "lab_drop = ['bicarbonate_max', 'chloride_max', 'calcium_max', 'magnesium_max', \n",
    "            'baseexcess_max', 'platelet_max', 'hemoglobin_max', 'phosphate_max', \n",
    "            'fibrinogen_max', 'ph_max', 'hematocrit_max', 'creatinine_min',\n",
    "            'bun_min', 'bilirubin_min', 'pt_min', 'inr_min', 'lactate_min', \n",
    "            'tropi_min', 'tropt_min', 'amylase_min', 'lipase_min', 'bnp_min',\n",
    "            'cpk_min', 'albumin_max', 'ioncalcium_max', 'pao2_max', 'pt_min',\n",
    "            'ptt_min', 'inr_min', 'aniongap_min']\n",
    "cohort = cohort.drop(lab_drop, axis=1)\n",
    "\n",
    "# sodium, deviates bidirectionally\n",
    "sodium_check = abs(cohort.sodium_min - 135.) >= abs(cohort.sodium_max - 145.)\n",
    "sodium = np.empty(len(cohort.index), dtype='float64')\n",
    "sodium[sodium_check] = cohort.sodium_min[sodium_check]\n",
    "sodium[~sodium_check] = cohort.sodium_max[~sodium_check]\n",
    "cohort = cohort.assign(sodium=sodium)\n",
    "cohort = cohort.drop(['sodium_min', 'sodium_max'], axis=1)\n",
    "\n",
    "# potassium, deviates bidirectionally, same treatment then\n",
    "potassium_check = abs(cohort.potassium_min - 3.5) >= abs(cohort.potassium_max - 5.0)\n",
    "potassium = np.empty(len(cohort.index), dtype='float64')\n",
    "potassium[potassium_check] = cohort.potassium_min[potassium_check]\n",
    "potassium[~potassium_check] = cohort.potassium_max[~potassium_check]\n",
    "cohort = cohort.assign(potassium=potassium)\n",
    "cohort = cohort.drop(['potassium_min', 'potassium_max'], axis=1)\n",
    "\n",
    "# similar treatment for glucose since hyperglycemia and hypoglycemia can both\n",
    "# be important dependent on the clinical context.\n",
    "glucose_check = abs(cohort.glucose_min - 70) >= abs(cohort.glucose_max - 130)\n",
    "glucose = np.empty(len(cohort.index), dtype='float64')\n",
    "glucose[glucose_check] = cohort.glucose_min[glucose_check]\n",
    "glucose[~glucose_check] = cohort.glucose_max[~glucose_check]\n",
    "cohort = cohort.assign(glucose=glucose)\n",
    "cohort = cohort.drop(['glucose_min', 'glucose_max'], axis=1)\n",
    "\n",
    "# wbc counts\n",
    "wbc_check = cohort.wbc_min < 2\n",
    "pmn_check = cohort.pmn_min < 45\n",
    "lym_check = cohort.lymphs_min < 20\n",
    "\n",
    "wbc = np.empty(len(cohort.index), dtype='float64')\n",
    "wbc[wbc_check] = cohort.wbc_min[wbc_check]\n",
    "wbc[~wbc_check] = cohort.wbc_max[~wbc_check]\n",
    "cohort = cohort.assign(wbc=wbc)\n",
    "cohort = cohort.drop(['wbc_min', 'wbc_max'], axis=1)\n",
    "\n",
    "pmn = np.empty(len(cohort.index), dtype='float64')\n",
    "pmn[pmn_check] = cohort.pmn_min[pmn_check]\n",
    "pmn[~pmn_check] = cohort.pmn_max[~pmn_check]\n",
    "cohort = cohort.assign(pmn=pmn)\n",
    "cohort = cohort.drop(['pmn_min', 'pmn_max'], axis=1)\n",
    "\n",
    "lym = np.empty(len(cohort.index), dtype='float64')\n",
    "lym[lym_check] = cohort.lymphs_min[lym_check]\n",
    "lym[~lym_check] = cohort.lymphs_max[~lym_check]\n",
    "cohort = cohort.assign(lym=lym)\n",
    "cohort = cohort.drop(['lymphs_min', 'lymphs_max'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of the data reveals some variables which are redundant and others which appear unreliable, and in some cases one source is better than another. As such we will:\n",
    "* Use aperiodic BP instead of periodic\n",
    "* Drop temperature; it is an unreliable signal when automatically captured\n",
    "* Drop APACHE elective surgery, we will simply classify patients by their admission diagnosis\n",
    "* Drop PaO2/FiO2 from APACHE and instead just use the PaO2 values derived directly from laboratory data\n",
    "* Use sCr from labs instead of APACHE table\n",
    "* Drop LoS variables since they would let our models peek into the future\n",
    "* Drop unit admit source, it was only included for the exclusion criteria\n",
    "* Drop APACHE score\n",
    "* Drop APACHE readmit and admission number, as they were only used to examine exclusion criteria\n",
    "* Missingness will be handled during modeling, but we should make sure that all missingness is labeled with `np.nan` and not -1 as is present in some of the eICU tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = cohort.replace(-1, np.nan)\n",
    "cohort = cohort.drop(['sbp_periodic_mean', 'dbp_periodic_mean', 'map_periodic_mean',\n",
    "                      'tempc_mean', 'apache_elect_surg', 'apache_creatinine', 'apache_pao2', \n",
    "                      'apache_fio2', 'apache_o2ratio', 'hospital_los', 'unit_los', \n",
    "                      'unitadmitsource', 'apache_score', 'apache_readmit', 'admission_num'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next turn to formatting the data so that it will be amenable to modeling. This entails converting categorical variables into indicators, and thus we must first convert the strings composing the categories into good variable names.\n",
    "\n",
    "We'll start with admission diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = cohort.drop('admit_diagnosis', axis=1)\n",
    "adx_dummies = pd.get_dummies(cohort.adx_group, 'adx', drop_first=True)\n",
    "cohort = pd.concat([cohort, adx_dummies], axis=1)\n",
    "cohort = cohort.drop('adx_group', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we turn to gender and ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_gender = (cohort.gender == 'Male').astype('int')\n",
    "cohort = cohort.assign(male_gender=male_gender)\n",
    "cohort = cohort.drop('gender', axis=1)\n",
    "\n",
    "eth_map = {'Caucasian' : 'caucasian', 'Other/Unknown' : 'other', \n",
    "           'Native American' : 'native_american', 'African American' : 'african_american',\n",
    "          'Asian' : 'asian', 'Hispanic' : 'hispanic', '' : 'other'}\n",
    "cohort.ethnicity = cohort.ethnicity.map(eth_map)\n",
    "eth_dummies = pd.get_dummies(cohort.ethnicity, 'eth', drop_first=True)\n",
    "cohort = pd.concat([cohort, eth_dummies], axis=1)\n",
    "cohort = cohort.drop('ethnicity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves unit type as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.unit_type = cohort.unit_type.str.replace('-', '_')\n",
    "cohort.unit_type = cohort.unit_type.str.replace(' ', '_')\n",
    "unit_dummies = pd.get_dummies(cohort.unit_type, 'unit', drop_first=True)\n",
    "cohort = pd.concat([cohort, unit_dummies], axis=1)\n",
    "cohort = cohort.drop('unit_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Save Train/Test Split of Features and Label\n",
    "\n",
    "We need to save the label and remove it from the features. We'll also need to save the APACHE prediction so as to incorporate the APACHE IVa model in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = (cohort.hospital_expiration == 'EXPIRED').astype('int')\n",
    "apache_pred = cohort.apache_prediction\n",
    "cohort = cohort.drop(['hospital_expiration', 'apache_prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can form a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y, train_apache, test_apache = train_test_split(cohort, label, apache_pred, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can save the CSV files corresponding to data frames we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.to_csv('./data/train_X.csv', index=False)\n",
    "train_y.to_csv('./data/train_y.csv', index=False, header=True)\n",
    "train_apache.to_csv('./data/train_apache.csv', index=False, header=True)\n",
    "\n",
    "test_X.to_csv('./data/test_X.csv', index=False)\n",
    "test_y.to_csv('./data/test_y.csv', index=False, header=True)\n",
    "test_apache.to_csv('./data/test_apache.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this portion complete, we can move onto the construction of our mortality models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
